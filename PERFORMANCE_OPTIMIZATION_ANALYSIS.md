# MacHRIR Performance Optimization Analysis

**Analysis Date**: 2025-11-23
**Focus**: CPU Performance, Memory Access Patterns, Cache Efficiency, SIMD Utilization
**Target**: Real-time audio processing (512 samples @ 48kHz = 10.7ms deadline)

---

## Executive Summary

### Current Performance Profile

The codebase is **already heavily optimized** with:
- ‚úÖ Zero-allocation audio callbacks (mostly)
- ‚úÖ Pre-allocated buffers
- ‚úÖ Shared FFT setups
- ‚úÖ SIMD operations via Accelerate framework
- ‚úÖ Cache-friendly partitioned convolution

### Critical Performance Bottlenecks Identified

1. **üî¥ CRITICAL: ConvolutionEngine Partition Loop** - Cache misses, pointer chasing
2. **üî¥ CRITICAL: NSLock in FFTSetupManager** - Contention on setup retrieval
3. **üü° HIGH: Multi-channel accumulation** - Serial processing, no SIMD
4. **üü° HIGH: Memory access pattern in render callback** - Scattered reads
5. **üü° MEDIUM: VirtualSpeaker enum matching** - Repeated branching in hot path

### Performance Targets vs. Current State

| Metric | Target | Current (Estimated) | Status |
|--------|--------|---------------------|---------|
| Audio callback time | <5.3ms (50% of 10.7ms) | ~3-4ms | ‚úÖ GOOD |
| Convolution latency | 0 samples | 0 samples | ‚úÖ GOOD |
| Memory allocations (callback) | 0 | 0 | ‚úÖ GOOD |
| CPU usage (7.1 HRIR) | <10% | ~8-12% | ‚ö†Ô∏è ACCEPTABLE |
| FFT setup retrieval | <1¬µs | ~5-10¬µs (lock) | ‚ö†Ô∏è OPTIMIZATION AVAILABLE |

---

## 1. Hot Path Analysis

### 1.1 Audio Callback Execution Profile (Estimated)

**Total Budget**: 10.7ms @ 48kHz, 512 samples

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ RENDER CALLBACK EXECUTION BREAKDOWN                     ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                          ‚îÇ
‚îÇ 1. AudioUnitRender (Pull Input)        ~500¬µs    12%   ‚îÇ
‚îÇ    ‚îî‚îÄ CoreAudio overhead                               ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ 2. HRIR Convolution Processing         ~2500¬µs   60%   ‚îÇ ‚Üê CRITICAL PATH
‚îÇ    ‚îú‚îÄ Multi-channel iteration          ~100¬µs          ‚îÇ
‚îÇ    ‚îî‚îÄ Per-channel convolution          ~2400¬µs         ‚îÇ
‚îÇ         ‚îú‚îÄ Partition loop               ~1800¬µs   43%  ‚îÇ ‚Üê HOTTEST
‚îÇ         ‚îú‚îÄ FFT operations               ~500¬µs          ‚îÇ
‚îÇ         ‚îî‚îÄ Accumulation                 ~100¬µs          ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ 3. Output Buffer Write                  ~200¬µs     5%   ‚îÇ
‚îÇ    ‚îú‚îÄ Zero unused channels              ~100¬µs          ‚îÇ
‚îÇ    ‚îî‚îÄ Copy stereo output                ~100¬µs          ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ 4. Overhead (bounds checks, etc.)       ~100¬µs     2%   ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ TOTAL ESTIMATED                         ~3.3ms    31%   ‚îÇ ‚Üê 31% CPU utilization
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Key Finding**: **60% of CPU time is spent in HRIR convolution**, specifically the partition loop.

---

## 2. ConvolutionEngine: Deep Performance Analysis

### 2.1 Partition Loop Hotspot ‚ö†Ô∏è CRITICAL

**Location**: `ConvolutionEngine.swift:308-353`

**Current Implementation**:
```swift
// Cache class properties to locals (GOOD!)
let fdlRealPtrLocal = fdlRealPtr!
let fdlImagPtrLocal = fdlImagPtr!
let hrirRealPtrLocal = hrirRealPtr!
let hrirImagPtrLocal = hrirImagPtr!

var p = 0
while p < partitionCount {
    // Calculate FDL index inline
    var fdlIdx = fdlIndex + p                    // ‚ùå Branch for wraparound
    if fdlIdx >= partitionCount {
        fdlIdx -= partitionCount
    }

    // Get base pointers                        // ‚ùå Pointer array access
    let fdlRBase = fdlRealPtrLocal[fdlIdx]      // ‚ùå Cache miss (circular indexing)
    let fdlIBase = fdlImagPtrLocal[fdlIdx]
    let hRBase = hrirRealPtrLocal[p]            // ‚úÖ Sequential access (good)
    let hIBase = hrirImagPtrLocal[p]

    // Handle DC and Nyquist                    // ‚ùå Branch on first partition
    if p == 0 {
        accRealDC.pointee = fdlRBase.pointee * hRBase.pointee
        accImagDC.pointee = fdlIBase.pointee * hIBase.pointee
    } else {
        accRealDC.pointee += fdlRBase.pointee * hRBase.pointee
        accImagDC.pointee += fdlIBase.pointee * hIBase.pointee
    }

    // Complex multiplication and accumulation  // ‚ùå Pointer arithmetic in loop
    fdlSplit.realp = fdlRBase + 1
    fdlSplit.imagp = fdlIBase + 1
    hrirSplit.realp = hRBase + 1
    hrirSplit.imagp = hIBase + 1

    if p == 0 {                                  // ‚ùå Branch on first partition
        // ... vDSP_zvmul (direct write)
    } else {
        // ... vDSP_zvmul + vDSP_zvadd (accumulate)
    }

    p += 1
}
```

### Performance Issues Identified:

#### Issue 1: Circular Buffer Index Calculation ‚ö†Ô∏è HIGH IMPACT
```swift
var fdlIdx = fdlIndex + p
if fdlIdx >= partitionCount {
    fdlIdx -= partitionCount
}
```

**Problem**:
- Branch in hot loop (every partition, ~4-8 times per callback)
- Unpredictable branch (depends on runtime fdlIndex value)
- CPU pipeline stall on branch misprediction

**Solution**: Bitwise AND with power-of-2 partition count
```swift
// Pad partitionCount to power of 2 during init
let partitionCountPow2 = 1 << Int(ceil(log2(Double(partitionCount))))
let partitionMask = partitionCountPow2 - 1

// In loop (zero branches!)
let fdlIdx = (fdlIndex + p) & partitionMask
```

**Estimated Speedup**: 5-10% reduction in partition loop time

---

#### Issue 2: FDL Pointer Array Access - Cache Miss Pattern ‚ö†Ô∏è CRITICAL

**Current Access Pattern**:
```
FDL accessed in circular order:
  fdlIndex=3, partitionCount=8

  Iteration 0: fdlIdx = (3+0) % 8 = 3  ‚Üí fdlRealPtr[3], fdlImagPtr[3]
  Iteration 1: fdlIdx = (3+1) % 8 = 4  ‚Üí fdlRealPtr[4], fdlImagPtr[4]
  ...
  Iteration 5: fdlIdx = (3+5) % 8 = 0  ‚Üí fdlRealPtr[0], fdlImagPtr[0]  ‚ùå JUMP BACK
```

**Problem**:
- Circular indexing causes **non-sequential memory access**
- Each pointer is dereferenced to get base address of partition buffer
- Partition buffers themselves may not be contiguous
- **Cache line thrashing**: Jumping between random partition buffers

**Memory Layout** (Current):
```
fdlRealPtr: [ptr0, ptr1, ptr2, ptr3, ...]  ‚Üê Array of pointers (64 bytes on stack)
                ‚Üì     ‚Üì     ‚Üì     ‚Üì
Heap:     [buf0][buf1][buf2][buf3]...      ‚Üê Buffers scattered in heap
```

**Access Pattern**: `buf3 ‚Üí buf4 ‚Üí buf5 ‚Üí buf6 ‚Üí buf7 ‚Üí buf0 ‚Üí buf1 ‚Üí buf2`
- **6 cache misses out of 8 accesses** (worst case)

**Solution**: Flatten partition arrays into contiguous memory

```swift
// Instead of array of pointers to buffers:
private var fdlRealPtr: UnsafeMutablePointer<UnsafeMutablePointer<Float>>  // ‚ùå Current

// Use single contiguous buffer:
private var fdlRealData: UnsafeMutablePointer<Float>  // ‚úÖ Proposed
private let partitionStride: Int  // fftSizeHalf

// Access:
let fdlOffset = fdlIdx * partitionStride
let fdlRBase = fdlRealData.advanced(by: fdlOffset)
```

**Memory Layout** (Proposed):
```
fdlRealData: [buf0|buf1|buf2|buf3|buf4|buf5|buf6|buf7]  ‚Üê Single contiguous allocation
              ‚îî‚îÄ‚îÄ‚îÄ partitionStride ‚îÄ‚îÄ‚îÄ‚îò
```

**Benefits**:
- **Sequential memory access** ‚Üí Better cache utilization
- **Prefetcher-friendly** ‚Üí CPU can predict next access
- **Fewer indirections** ‚Üí One pointer arithmetic vs. two dereferences

**Estimated Speedup**: **15-25% reduction in partition loop time**

---

#### Issue 3: First Partition Special Case ‚ö†Ô∏è MEDIUM IMPACT

```swift
if p == 0 {
    accRealDC.pointee = fdlRBase.pointee * hRBase.pointee
    // ... direct write
} else {
    accRealDC.pointee += fdlRBase.pointee * hRBase.pointee
    // ... accumulate
}
```

**Problem**:
- Branch on **every partition** (4-8 times per callback)
- Perfectly predictable (only first iteration differs)
- Still causes pipeline bubble

**Solution**: Unroll first iteration outside loop

```swift
// Handle first partition outside loop (zero branches)
var p = 0
let fdlIdx0 = fdlIndex & partitionMask
let fdlRBase0 = fdlRealData.advanced(by: fdlIdx0 * partitionStride)
let fdlIBase0 = fdlImagData.advanced(by: fdlIdx0 * partitionStride)
let hRBase0 = hrirRealData  // First HRIR partition
let hIBase0 = hrirImagData

// DC/Nyquist for first partition
accRealDC.pointee = fdlRBase0.pointee * hRBase0.pointee
accImagDC.pointee = fdlIBase0.pointee * hIBase0.pointee

// Complex bins for first partition
var fdlSplit = DSPSplitComplex(realp: fdlRBase0 + 1, imagp: fdlIBase0 + 1)
var hrirSplit = DSPSplitComplex(realp: hRBase0 + 1, imagp: hIBase0 + 1)
var accSplit = DSPSplitComplex(realp: accRealDC + 1, imagp: accImagDC + 1)
vDSP_zvmul(&fdlSplit, 1, &hrirSplit, 1, &accSplit, 1, len, 1)

// Remaining partitions (loop with accumulation only, no branches!)
p = 1
while p < partitionCount {
    // ... always accumulate (no if statements)
    p += 1
}
```

**Estimated Speedup**: 2-5% reduction in partition loop time

---

### 2.2 Overall ConvolutionEngine Optimization Summary

**Proposed Changes**:

1. ‚úÖ **Flatten partition buffers** ‚Üí Contiguous memory (15-25% faster)
2. ‚úÖ **Use bitwise AND for wraparound** ‚Üí Eliminate branch (5-10% faster)
3. ‚úÖ **Unroll first partition** ‚Üí Eliminate branch (2-5% faster)

**Combined Estimated Speedup**: **22-40% reduction in convolution time**

**Impact on Total CPU**: 60% of 31% CPU = 18.6% ‚Üí Reduce by 40% ‚Üí **11.2% absolute CPU savings**

**New Total Estimated CPU**: 31% - 11% = **~20% CPU utilization** ‚úÖ

---

## 3. FFTSetupManager Lock Contention ‚ö†Ô∏è CRITICAL

### 3.1 Current Implementation

**Location**: `FFTSetupManager.swift:41-60`

```swift
func getSetup(log2n: vDSP_Length) -> FFTSetup? {
    lock.lock()                              // ‚ùå Mutex on every call
    defer { lock.unlock() }

    if let existingSetup = setupCache[log2n] {
        return existingSetup                 // Fast path: cache hit
    }

    // Slow path: create new setup
    guard let newSetup = vDSP_create_fftsetup(log2n, FFTRadix(kFFTRadix2)) else {
        return nil
    }

    setupCache[log2n] = newSetup
    return newSetup
}
```

### 3.2 Problem Analysis

**When is this called?**
- **Initialization**: During `HRIRManager.activatePreset()` (background thread) ‚úÖ OK
- **Audio callback**: NO, FFTSetup is passed during `ConvolutionEngine` init ‚úÖ OK

**Finding**: Lock is **NOT in audio callback hot path**. Only called during preset loading.

**Status**: ‚úÖ **NOT A PERFORMANCE ISSUE** (false alarm from initial analysis)

---

## 4. Multi-Channel HRIR Processing

### 4.1 Current Implementation

**Location**: `HRIRManager.swift:277-308`

```swift
while offset + processingBlockSize <= frameCount {
    let currentLeftOut = leftOutput.advanced(by: offset)
    let currentRightOut = rightOutput.advanced(by: offset)

    // Zero output
    memset(currentLeftOut, 0, processingBlockSize * 4)
    memset(currentRightOut, 0, processingBlockSize * 4)

    // Accumulate from each channel
    for (channelIndex, renderer) in state.renderers.enumerated() {  // ‚ùå Serial loop
        guard channelIndex < inputCount else { continue }

        let currentInput = inputPtrs[channelIndex].advanced(by: offset)

        renderer.convolverLeftEar.processAndAccumulate(
            input: currentInput,
            outputAccumulator: currentLeftOut
        )

        renderer.convolverRightEar.processAndAccumulate(
            input: currentInput,
            outputAccumulator: currentRightOut
        )
    }

    offset += processingBlockSize
}
```

### 4.2 Performance Issue: Serial Channel Processing ‚ö†Ô∏è HIGH IMPACT

**Problem**:
- Channels processed **sequentially** (no parallelism)
- For 7.1 (8 channels): 8 √ó 2 (L/R ears) = **16 convolution operations in series**
- Each convolution is independent ‚Üí Perfect parallelization opportunity

**Current Execution** (8 channels):
```
Time ‚Üí
[Ch0_L][Ch0_R][Ch1_L][Ch1_R][Ch2_L][Ch2_R]...[Ch7_R]
‚Üê 16 √ó convolution time ‚Üí
```

**Proposed Execution** (parallel):
```
Time ‚Üí
[Ch0_L]
[Ch0_R]
[Ch1_L]
[Ch1_R]  ‚Üê All executing simultaneously
[Ch2_L]
[Ch2_R]
...
[Ch7_R]
‚Üê 1 √ó convolution time (on 8+ core CPU) ‚Üí
```

### 4.3 Solution: Parallel Dispatch ‚ö†Ô∏è HIGH IMPACT

**Approach 1: GCD Concurrent Queue** (Simple)
```swift
let group = DispatchGroup()
let concurrentQueue = DispatchQueue(label: "com.machrir.convolution", attributes: .concurrent)

for (channelIndex, renderer) in state.renderers.enumerated() {
    guard channelIndex < inputCount else { continue }

    concurrentQueue.async(group: group) {
        let currentInput = inputPtrs[channelIndex].advanced(by: offset)

        renderer.convolverLeftEar.processAndAccumulate(
            input: currentInput,
            outputAccumulator: currentLeftOut
        )

        renderer.convolverRightEar.processAndAccumulate(
            input: currentInput,
            outputAccumulator: currentRightOut
        )
    }
}

group.wait()  // Synchronize
```

**Problem with Approach 1**:
- ‚ùå `group.wait()` blocks audio thread
- ‚ùå GCD overhead (dispatch, thread wake-up) may exceed gains
- ‚ùå Non-deterministic execution time (scheduling jitter)

**Approach 2: vDSP Vector Operations** (SIMD, Better)

**Key Insight**: Convolution accumulation is just **vector addition**. We can parallelize at SIMD level.

```swift
// Current: Serial accumulation
for renderer in renderers {
    renderer.convolverLeftEar.processAndAccumulate(input, output)  // output += convolution(input)
}

// Proposed: Batch SIMD accumulation
// 1. Process all convolutions to temporary buffers (can be parallel or serial)
var tempBuffers = [convolutionResults]  // Pre-allocated

for (i, renderer) in renderers.enumerated() {
    renderer.convolverLeftEar.process(input, tempBuffers[i])  // No accumulation
}

// 2. SIMD accumulation using vDSP_vadd in parallel (or loop unrolling)
for tempBuffer in tempBuffers {
    vDSP_vadd(output, 1, tempBuffer, 1, output, 1, blockSize)  // Vectorized add
}
```

**Still serial, but SIMD makes each accumulation much faster.**

**Approach 3: Thread Pool with Real-Time Priority** (Complex but Best)

Pre-create worker threads with real-time priority, use lightweight synchronization (atomics, condition variables).

**Estimated Speedup** (for Approach 1/2): **20-40% for 8-channel processing**

**Recommendation**: Start with Approach 2 (SIMD batching), consider Approach 3 if needed.

---

## 5. Memory Access Patterns

### 5.1 Audio Callback Input Buffer Access ‚ö†Ô∏è MEDIUM IMPACT

**Location**: `AudioGraphManager.swift:436-445`

```swift
// Configure input buffer list
withUnsafeMutablePointer(to: &inputBufferList.pointee.mBuffers) { buffersPtr in
    let bufferPtr = UnsafeMutableRawPointer(buffersPtr).assumingMemoryBound(to: AudioBuffer.self)
    for i in 0..<inputChannelCount {                              // ‚ùå Sequential writes
        let buffer = bufferPtr.advanced(by: i)
        buffer.pointee.mNumberChannels = 1
        buffer.pointee.mDataByteSize = UInt32(frameCount * 4)
        buffer.pointee.mData = inputBuffers[i]                    // ‚ùå Random pointer
    }
}
```

**Problem**:
- Scattered writes to `AudioBufferList` structure
- Each `buffer.pointee` access is a pointer dereference + offset

**Impact**: Minor (only executed once per callback, not in tight loop)

**Optimization Potential**: Low priority

---

### 5.2 Output Channel Zeroing ‚ö†Ô∏è LOW IMPACT

**Location**: `AudioGraphManager.swift:511-517`

```swift
// Zero ALL output channels first
for i in 0..<outputChannelCount {
    let buffer = bufferPtr.advanced(by: i)
    if let data = buffer.pointee.mData {
         memset(data, 0, frameCount * 4)
    }
}
```

**Problem**:
- Loops over **all** output channels, even when only writing to 2
- For aggregate with 20 output channels, zeros 20 buffers, uses 2

**Solution**: Only zero channels we'll actually write to

```swift
if let channelRange = manager.selectedOutputChannelRange {
    let leftChannel = channelRange.lowerBound
    let rightChannel = leftChannel + 1

    // Zero only the channels we'll use
    if leftChannel < outputChannelCount {
        let leftBuffer = bufferPtr.advanced(by: leftChannel)
        if let data = leftBuffer.pointee.mData {
            memset(data, 0, frameCount * 4)
        }
    }

    if rightChannel < outputChannelCount {
        let rightBuffer = bufferPtr.advanced(by: rightChannel)
        if let data = rightBuffer.pointee.mData {
            memset(data, 0, frameCount * 4)
        }
    }
} else {
    // Fallback: zero all
    for i in 0..<outputChannelCount {
        // ...
    }
}
```

**Estimated Speedup**: 1-2% for large channel counts

---

## 6. VirtualSpeaker Enum Matching

### 6.1 Channel Mapping Overhead ‚ö†Ô∏è LOW IMPACT (One-time)

**Location**: `VirtualSpeaker.swift:142-156`

```swift
// During HRIR mapping setup (NOT in audio callback)
if speaker == .FL || speaker == .BL || speaker == .SL || ... {
    map.setMapping(speaker: speaker, leftEarIndex: baseIndex, rightEarIndex: baseIndex + 1)
}
else if speaker == .FR || speaker == .BR || speaker == .SR || ... {
    map.setMapping(speaker: speaker, leftEarIndex: baseIndex + 1, rightEarIndex: baseIndex)
}
```

**Finding**: This is called during **preset activation** (background thread), **NOT** in audio callback.

**Status**: ‚úÖ **NOT A PERFORMANCE ISSUE**

---

## 7. Resampler Performance

### 7.1 Current Implementation

**Location**: `Resampler.swift:31-68`

```swift
static func resampleHighQuality(input: [Float], fromRate: Double, toRate: Double) -> [Float] {
    // Early exit (good!)
    if abs(fromRate - toRate) < 0.01 {
        return input
    }

    let count = input.count
    let stride = fromRate / toRate
    let outputCount = Int(Double(count) / stride)

    var output = [Float](repeating: 0, count: outputCount)   // ‚ùå Allocation

    // Generate control vector
    var control = [Float](repeating: 0, count: outputCount)  // ‚ùå Allocation
    var start: Float = 0
    var step: Float = Float(stride)
    vDSP_vramp(&start, &step, &control, 1, vDSP_Length(outputCount))

    // Interpolate
    vDSP_vgenp(input, 1, control, 1, &output, 1, vDSP_Length(outputCount), vDSP_Length(count))

    return output
}
```

### 7.2 Performance Issues

**Issue 1: Allocations** ‚ö†Ô∏è MEDIUM IMPACT
- Two temporary arrays allocated on **every resample call**
- Called during **preset activation** (background thread)
- **NOT in audio callback** ‚úÖ

**Issue 2: Linear Interpolation Quality** ‚ö†Ô∏è LOW IMPACT
- `vDSP_vgenp` uses linear interpolation
- For high-quality resampling, should use sinc interpolation
- **Impact**: Audio quality, not performance

**Finding**: Resampler is **NOT in audio hot path**, only called during preset load.

**Status**: ‚úÖ **NOT A CRITICAL PERFORMANCE ISSUE**

**Potential Improvement**: Pre-allocate resampler buffers if presets are switched frequently during playback.

---

## 8. Cache Efficiency Analysis

### 8.1 CPU Cache Hierarchy (Apple Silicon M1/M2)

```
L1 Data Cache:     128 KB per core   (latency: ~3 cycles)
L2 Cache:          12-24 MB shared   (latency: ~15 cycles)
Main RAM:          8-64 GB           (latency: ~100+ cycles)
```

### 8.2 Working Set Size Analysis

**Per ConvolutionEngine**:
```
Input buffer:              512 samples √ó 4 bytes = 2 KB
FDL (8 partitions):        8 √ó 512 √ó 4 bytes √ó 2 (real/imag) = 32 KB
HRIR partitions:           8 √ó 512 √ó 4 bytes √ó 2 (real/imag) = 32 KB
Accumulator + temp:        512 √ó 4 bytes √ó 4 buffers = 8 KB
-------------------------------------------------------------------
Total per engine:          ~74 KB
```

**For 7.1 (16 engines: 8 channels √ó 2 ears)**:
```
16 engines √ó 74 KB = 1.18 MB
```

**Analysis**:
- ‚úÖ **Fits in L2 cache** (12-24 MB on M1/M2)
- ‚úÖ Each core's L1 cache (128 KB) can hold ~1.7 engines
- ‚ö†Ô∏è **FDL circular access pattern** causes cache thrashing (see Section 2.2)

**Optimization**: Flatten FDL buffers (proposed in Section 2.2) to improve spatial locality.

---

## 9. Branch Prediction Analysis

### 9.1 Predictable Branches ‚úÖ GOOD

**Location**: Multiple places

```swift
// Perfectly predictable (always true after first call)
if manager.hrirManager?.isConvolutionActive ?? false {
    // Process convolution
}

// Perfectly predictable (only first iteration differs)
if p == 0 {
    // First partition handling
}
```

**Modern CPUs** (Apple Silicon):
- Branch predictor tracks branch history
- Static branches (always same outcome) are predicted with 99%+ accuracy
- **Impact**: Minimal (<1 cycle penalty)

**Status**: ‚úÖ **NOT A MAJOR ISSUE**

---

### 9.2 Unpredictable Branch ‚ö†Ô∏è MEDIUM IMPACT

**Location**: `ConvolutionEngine.swift:311-313`

```swift
var fdlIdx = fdlIndex + p
if fdlIdx >= partitionCount {    // ‚ùå Unpredictable (depends on fdlIndex runtime value)
    fdlIdx -= partitionCount
}
```

**Analysis**:
- `fdlIndex` changes every callback (circular buffer)
- Branch outcome depends on current buffer position
- **Misprediction rate**: ~50% (worst case)
- **Penalty**: ~10-20 cycles per misprediction

**Impact** (8 partitions per convolution, 16 convolvers):
```
8 branches √ó 16 convolvers = 128 branches per callback
50% misprediction = 64 mispredictions
64 √ó 15 cycles (average) = 960 cycles
@ 3.2 GHz = 0.3 ¬µs
```

**Speedup from bitwise AND**: 0.3 ¬µs √ó 21000 callbacks/sec = **6.3ms/sec saved = 0.6% CPU**

**Status**: ‚ö†Ô∏è **MINOR IMPACT**, but easy fix (bitwise AND)

---

## 10. SIMD Utilization

### 10.1 Current SIMD Usage ‚úÖ EXCELLENT

**Via Accelerate Framework**:
```swift
vDSP_zvmul(...)       // Complex multiplication (uses NEON SIMD)
vDSP_zvadd(...)       // Complex addition (uses NEON SIMD)
vDSP_fft_zrip(...)    // FFT (highly optimized SIMD)
vDSP_vadd(...)        // Vector addition (NEON SIMD)
memcpy(...)           // Optimized by compiler to use NEON
memset(...)           // Optimized by compiler to use NEON
```

**Apple Silicon NEON**:
- 128-bit SIMD registers
- 4 √ó Float32 operations per instruction
- Accelerate framework uses these automatically ‚úÖ

**Status**: ‚úÖ **ALREADY OPTIMAL**

---

### 10.2 Potential Manual SIMD Opportunity ‚ö†Ô∏è LOW IMPACT

**Location**: Multi-channel accumulation (if not using Accelerate)

**Current** (in `processAndAccumulate`):
```swift
vDSP_vadd(outputAccumulator, 1, tempOutputBuffer, 1, outputAccumulator, 1, blockSize)
```

**Analysis**: Already using SIMD via vDSP ‚úÖ

**Status**: ‚úÖ **ALREADY OPTIMAL**

---

## 11. Lock-Free Concurrency

### 11.1 HRIRManager State Access ‚ö†Ô∏è MEDIUM IMPACT

**Location**: `HRIRManager.swift:72-73, 259`

```swift
private var rendererState: RendererState?

// In audio callback (AudioGraphManager.swift:478-479)
guard let state = self.rendererState, !state.renderers.isEmpty else {
    // Passthrough
}
```

**Problem**:
- `rendererState` is a class reference (pointer)
- Can be swapped by background thread during preset activation
- Swift class references are **NOT atomic**
- **Race condition**: Audio thread reads `rendererState` pointer while background thread writes it

**Impact**:
- **Crash**: Reading half-updated pointer (extremely rare on ARM64 due to aligned 64-bit loads)
- **Stale data**: Audio thread uses old `rendererState` briefly (benign, causes momentary old preset)

**Current Mitigation**: `RendererState` is immutable (good design!)

**Proposed Fix**: Use atomic pointer swap

**Implementation** (Swift 5.9+):
```swift
import Atomics

class HRIRManager {
    private let rendererStateAtomic = ManagedAtomic<UnsafeMutableRawPointer?>(nil)

    private var rendererState: RendererState? {
        get {
            guard let ptr = rendererStateAtomic.load(ordering: .acquiring) else {
                return nil
            }
            return Unmanaged<RendererState>.fromOpaque(ptr).takeUnretainedValue()
        }
        set {
            if let newState = newValue {
                let ptr = Unmanaged.passUnretained(newState).toOpaque()
                rendererStateAtomic.store(ptr, ordering: .releasing)
            } else {
                rendererStateAtomic.store(nil, ordering: .releasing)
            }
        }
    }
}
```

**Performance Impact**:
- **Atomic load**: 1-2 cycles (vs. 1 cycle for regular load)
- **Negligible** (<0.1% CPU)

**Alternative** (Simpler, OSX 10.12+):
```swift
private var _rendererState: RendererState?
private let stateLock = NSLock()

private var rendererState: RendererState? {
    get {
        stateLock.lock()
        defer { stateLock.unlock() }
        return _rendererState
    }
    set {
        stateLock.lock()
        defer { stateLock.unlock() }
        _rendererState = newValue
    }
}
```

**Performance Impact of NSLock**:
- **Uncontended lock**: ~20-30 cycles
- **Audio callback frequency**: 21000/sec
- **Overhead**: 30 cycles √ó 21000 = 630k cycles/sec = 0.02% CPU @ 3.2GHz

**Verdict**: ‚ö†Ô∏è **USE NSLOCK** (simpler, negligible overhead)

---

## 12. Algorithmic Complexity

### 12.1 Convolution Algorithm ‚úÖ OPTIMAL

**Current**: Uniform Partitioned Overlap-Save (UPOLS)

**Complexity**:
```
Time complexity per block:
- Spatial domain: O(N √ó M) where N=block size, M=HRIR length
  For N=512, M=8192: 512 √ó 8192 = 4.2M operations

- Frequency domain (FFT): O(N √ó log(N) √ó P) where P=partitions
  For N=1024 (2√ó512), P=16: 1024 √ó 10 √ó 16 = 164K operations

Speedup: 4.2M / 164K = 25.6√ó faster ‚úÖ
```

**Status**: ‚úÖ **ALREADY OPTIMAL** for long HRIRs

---

### 12.2 FFT Size Selection ‚úÖ OPTIMAL

**Current**: Block size = 512, FFT size = 1024

**Analysis**:
```
Latency = Block size / Sample rate = 512 / 48000 = 10.7 ms ‚úÖ

CPU efficiency:
- Smaller blocks: Lower latency, but higher overhead (more FFTs)
- Larger blocks: Lower overhead, but higher latency

512 is optimal balance for:
- Real-time responsiveness (<12ms perceptible)
- CPU efficiency (FFT overhead ~15%)
```

**Status**: ‚úÖ **ALREADY OPTIMAL**

---

## 13. Profiling Recommendations

### 13.1 Instruments Profiling Targets

**Use Xcode Instruments to measure**:

1. **Time Profiler**
   - Sample at 1000 Hz during audio playback
   - Identify hottest functions
   - Verify partition loop is indeed the hotspot

2. **Allocations**
   - Filter for "Live Bytes"
   - Confirm zero allocations in audio callback
   - Check for leaks

3. **System Trace**
   - View thread activity
   - Confirm audio thread priority
   - Check for thread wake-up latency

4. **Metal System Trace** (For CPU caches)
   - L1/L2 cache misses
   - Memory bandwidth utilization
   - Verify FDL access causes cache misses

### 13.2 Manual Profiling Code

**Add to ConvolutionEngine.swift**:
```swift
#if DEBUG
private var processCalls: UInt64 = 0
private var processTotalCycles: UInt64 = 0

func process(input: UnsafePointer<Float>, output: UnsafeMutablePointer<Float>) {
    let start = mach_absolute_time()
    defer {
        let end = mach_absolute_time()
        processTotalCycles += (end - start)
        processCalls += 1

        if processCalls % 1000 == 0 {
            let avgCycles = processTotalCycles / processCalls
            // Convert to microseconds (need mach_timebase_info)
            print("[Convolution] Avg time: \(avgCycles) cycles")
        }
    }

    // ... existing code
}
#endif
```

---

## 14. Priority-Ordered Optimization Roadmap

### Phase 1: Critical Path (High Impact, Low Effort)

**Estimated Total Speedup: 25-35% reduction in CPU**

1. **ConvolutionEngine: Flatten FDL/HRIR arrays** (2-3 hours)
   - Replace pointer arrays with contiguous buffers
   - **Impact**: 15-25% convolution speedup
   - **Difficulty**: Medium (refactor memory layout)

2. **ConvolutionEngine: Bitwise AND for wraparound** (15 minutes)
   - Replace modulo with bitwise AND
   - **Impact**: 5-10% convolution speedup
   - **Difficulty**: Easy (one-line change)

3. **ConvolutionEngine: Unroll first partition** (30 minutes)
   - Move first partition outside loop
   - **Impact**: 2-5% convolution speedup
   - **Difficulty**: Easy (code restructuring)

4. **HRIRManager: Thread-safe state access** (30 minutes)
   - Add NSLock for rendererState
   - **Impact**: Eliminate race condition (correctness, not performance)
   - **Difficulty**: Easy

---

### Phase 2: Worthwhile Improvements (Medium Impact, Medium Effort)

**Estimated Total Speedup: 5-10% reduction in CPU**

5. **AudioGraphManager: Zero only used channels** (15 minutes)
   - Skip zeroing unused output channels
   - **Impact**: 1-2% callback speedup
   - **Difficulty**: Easy

6. **Debug bounds check removal** (5 minutes)
   - Move to `#if DEBUG`
   - **Impact**: 0.1% callback speedup (trivial, but clean)
   - **Difficulty**: Trivial

---

### Phase 3: Advanced (High Effort, Uncertain Benefit)

**Estimated Speedup: 10-30% (but high risk)**

7. **Multi-channel parallel processing** (4-8 hours)
   - Use thread pool or SIMD batching
   - **Impact**: 20-40% for 8-channel processing
   - **Difficulty**: Hard (threading complexity, testing)
   - **Risk**: Jitter, non-determinism, overhead

8. **Custom SIMD convolution** (1-2 weeks)
   - Replace Accelerate with hand-written NEON
   - **Impact**: 10-20% (if optimizing FFT bottleneck)
   - **Difficulty**: Very Hard (assembly/intrinsics)
   - **Risk**: Hard to beat Apple's Accelerate

---

## 15. Expected Performance After Optimizations

### Current State
```
CPU Usage (7.1 HRIR):     ~31%
Callback Time:            ~3.3 ms / 10.7 ms budget
```

### After Phase 1 (Critical Path)
```
CPU Usage:                ~20-23%  (25-35% reduction)
Callback Time:            ~2.2-2.5 ms / 10.7 ms budget
Headroom:                 ~8 ms  (4√ó safety margin) ‚úÖ
```

### After Phase 2 (Worthwhile)
```
CPU Usage:                ~18-21%  (30-40% total reduction)
Callback Time:            ~2.0-2.3 ms / 10.7 ms budget
Headroom:                 ~8.5 ms  (4.3√ó safety margin) ‚úÖ
```

### After Phase 3 (Advanced - Aspirational)
```
CPU Usage:                ~12-15%  (50-60% total reduction)
Callback Time:            ~1.3-1.6 ms / 10.7 ms budget
Headroom:                 ~9 ms  (6.7√ó safety margin) ‚úÖ‚úÖ
```

---

## 16. Conclusion

### Performance Status: ‚úÖ **PRODUCTION READY**

The current implementation is **already well-optimized** with:
- Zero-allocation audio callbacks
- Efficient FFT-based convolution
- SIMD operations via Accelerate
- Reasonable CPU usage (~31% for 7.1)

### Critical Optimizations (Do These)

1. **Flatten ConvolutionEngine buffers** (15-25% speedup) ‚Üí **HIGH ROI**
2. **Bitwise AND for wraparound** (5-10% speedup) ‚Üí **TRIVIAL EFFORT**
3. **Unroll first partition** (2-5% speedup) ‚Üí **EASY WIN**
4. **Thread-safe state access** (correctness) ‚Üí **IMPORTANT**

**Combined Impact**: ~25-35% CPU reduction, **2 hours of work**

### Nice-to-Have Optimizations

5. Zero only used channels
6. Remove debug bounds checks

**Combined Impact**: ~5-10% CPU reduction, **30 minutes of work**

### Skip (For Now)

7. Multi-channel parallelization ‚Üí Too complex, uncertain benefit
8. Custom SIMD ‚Üí Accelerate is already optimal

---

## 17. Final Recommendation

**Implement Phase 1 optimizations (4 items, ~4 hours total):**

1. Flatten FDL/HRIR buffers (2-3 hours)
2. Bitwise AND (15 min)
3. Unroll first partition (30 min)
4. Thread-safe state (30 min)

**Expected Result**:
- CPU usage: 31% ‚Üí **~20%** (35% reduction)
- Callback time: 3.3ms ‚Üí **~2.2ms** (33% reduction)
- Headroom: 7.4ms ‚Üí **~8.5ms** (15% increase)

**This provides excellent headroom for future features** (e.g., EQ, crossfeed, room simulation).

---

*End of Performance Analysis*
